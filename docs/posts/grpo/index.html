<!doctype html>
<html
  dir="ltr"
  lang="en"
  data-theme=""
  
    class="html theme--light"
  
><head>
  <meta charset="utf-8" />
  <meta name="google-site-verification" content="-B7y37dKor8vKcEE0aVwV1Fgx_YJrRCpQtqjN8LidhA" />
  <title>
    Katrina Drozdov
        |
        9 Lessons I Learned while Doing RL Post-Training for LLMs
      

    

  </title>

  <meta name="generator" content="Hugo 0.145.0"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover" />
  <meta name="author" content="Katrina Drozdov" />
  <meta
    name="description"
    content="AI Researcher"
  />
  
  
    
    
    <link
      rel="stylesheet"
      href="/scss/main.min.9bf285e9a59bc84fa36d9bcd5266be4303a21c43613cddc50596b1201e24c5f3.css"
      integrity="sha256-m/KF6aWbyE&#43;jbZvNUma&#43;QwOiHENhPN3FBZaxIB4kxfM="
      crossorigin="anonymous"
      type="text/css"
    />
  

  
  <link
    rel="stylesheet"
    href="/css/markupHighlight.min.73ccfdf28df555e11009c13c20ced067af3cb021504cba43644c705930428b00.css"
    integrity="sha256-c8z98o31VeEQCcE8IM7QZ688sCFQTLpDZExwWTBCiwA="
    crossorigin="anonymous"
    type="text/css"
  />
  
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/fontawesome.min.137b1cf3cea9a8adb7884343a9a5ddddf4280f59153f74dc782fb7f7bf0d0519.css"
    integrity="sha256-E3sc886pqK23iENDqaXd3fQoD1kVP3TceC&#43;3978NBRk="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/solid.min.e65dc5b48fb5f39b142360c57c3a215744c94e56c755c929cc3e88fe12aab4d3.css"
    integrity="sha256-5l3FtI&#43;185sUI2DFfDohV0TJTlbHVckpzD6I/hKqtNM="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/regular.min.6f4f16d58da1c82c0c3a3436e021a3d39b4742f741192c546e73e947eacfd92f.css"
    integrity="sha256-b08W1Y2hyCwMOjQ24CGj05tHQvdBGSxUbnPpR&#43;rP2S8="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/brands.min.e10425ad768bc98ff1fb272a0ac8420f9d1ba22f0612c08ff1010c95080ffe7e.css"
    integrity="sha256-4QQlrXaLyY/x&#43;ycqCshCD50boi8GEsCP8QEMlQgP/n4="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.1/css/academicons.min.css" rel="stylesheet">


  <link rel="canonical" href="https://kevtimova.github.io/posts/grpo/" />

  
  
  
  
  <script
    type="text/javascript"
    src="/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js"
    integrity="sha256-&#43;RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI="
    crossorigin="anonymous"
  ></script>

  
    
    
    <script
      type="text/javascript"
      src="/js/anatole-theme-switcher.min.d6d329d93844b162e8bed1e915619625ca91687952177552b9b3e211014a2957.js"
      integrity="sha256-1tMp2ThEsWLovtHpFWGWJcqRaHlSF3VSubPiEQFKKVc="
      crossorigin="anonymous"
    ></script>
  

  

  


  
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="9 Lessons I Learned while Doing RL Post-Training for LLMs">
  <meta name="twitter:description" content="I recently had the chance to experiment with post-training techniques for large language models, a space that has become central to making LLMs useful and controllable in real-world applications. I used Group Relative Policy Optimization (GRPO) from the open-source open-r1 repository, fine-tuning Qwen2.5-7B-Instruct on the coding subset of the Mixture-of-Thoughts dataset.">



  
  <meta property="og:url" content="https://kevtimova.github.io/posts/grpo/">
  <meta property="og:site_name" content="Katrina Drozdov&#39;s Website">
  <meta property="og:title" content="9 Lessons I Learned while Doing RL Post-Training for LLMs">
  <meta property="og:description" content="I recently had the chance to experiment with post-training techniques for large language models, a space that has become central to making LLMs useful and controllable in real-world applications. I used Group Relative Policy Optimization (GRPO) from the open-source open-r1 repository, fine-tuning Qwen2.5-7B-Instruct on the coding subset of the Mixture-of-Thoughts dataset.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-07-08T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-07-08T00:00:00+00:00">



  
  
  
  
  <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "articleSection": "posts",
        "name": "9 Lessons I Learned while Doing RL Post-Training for LLMs",
        "headline": "9 Lessons I Learned while Doing RL Post-Training for LLMs",
        "alternativeHeadline": "",
        "description": "
      
        \u003cp\u003eI recently had the chance to experiment with post-training techniques for large language models, a space that has become central to making LLMs useful and controllable in real-world applications. I used \u003ca href=\u0022https:\/\/arxiv.org\/abs\/2402.03300\u0022\u003eGroup Relative Policy Optimization (GRPO)\u003c\/a\u003e from the open-source \u003ca href=\u0022https:\/\/github.com\/huggingface\/open-r1\u0022\u003e\u003ccode\u003eopen-r1\u003c\/code\u003e\u003c\/a\u003e repository, fine-tuning \u003ca href=\u0022https:\/\/huggingface.co\/Qwen\/Qwen2.5-7B-Instruct\u0022\u003eQwen2.5-7B-Instruct\u003c\/a\u003e on the coding subset of the \u003ca href=\u0022https:\/\/huggingface.co\/datasets\/open-r1\/Mixture-of-Thoughts\u0022\u003eMixture-of-Thoughts\u003c\/a\u003e dataset.\u003c\/p\u003e


      


    ",
        "inLanguage": "en",
        "isFamilyFriendly": "true",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/kevtimova.github.io\/posts\/grpo\/"
        },
        "author" : {
            "@type": "Person",
            "name": "Katrina Drozdov"
        },
        "creator" : {
            "@type": "Person",
            "name": "Katrina Drozdov"
        },
        "accountablePerson" : {
            "@type": "Person",
            "name": "Katrina Drozdov"
        },
        "copyrightHolder" : {
            "@type": "Person",
            "name": "Katrina Drozdov"
        },
        "copyrightYear" : "2025",
        "dateCreated": "2025-07-08T00:00:00.00Z",
        "datePublished": "2025-07-08T00:00:00.00Z",
        "dateModified": "2025-07-08T00:00:00.00Z",
        "publisher":{
            "@type":"Organization",
            "name": "Katrina Drozdov",
            "url": "https://kevtimova.github.io/",
            "logo": {
                "@type": "ImageObject",
                "url": "https:\/\/kevtimova.github.io\/favicon-32x32.png",
                "width":"32",
                "height":"32"
            }
        },
        "image": 
      [
      ]

    ,
        "url" : "https:\/\/kevtimova.github.io\/posts\/grpo\/",
        "wordCount" : "721",
        "genre" : [ ],
        "keywords" : [ ]
    }
  </script>



  
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-5WXBKGXQT7"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-5WXBKGXQT7');
    </script>
    
</head>
<body class="body">
    <div class="wrapper">
      <aside
        
          class="wrapper__sidebar"
        
      ><div
  class="sidebar
    animated fadeInDown
  "
>
  <div class="sidebar__content">
    <div class="sidebar__introduction">
      <img
        class="sidebar__introduction-profileimage"
        src="/images/profile.jpg"
        alt="profile picture"
      />
      <div class="sidebar__introduction-title">
        <h1>
          <a href="/">Katrina Drozdov</a>
        </h1>
        
        <h3>(Evtimova)</h3>
        
      </div>
      <div class="sidebar__introduction-description">
        <h2>AI Researcher</h2>
      </div>
    </div>
    
    <ul class="sidebar__list">
      
        <li class="sidebar__list-item">
          <a
            href="/#contact"
            target="_blank"
            rel="noopener me"
            aria-label="Email"
            title="Email"
          >
            
              <i class="fas fa-envelope fa-2x" aria-hidden="true"></i>
            
          </a>
        </li>
      
        <li class="sidebar__list-item">
          <a
            href="https://scholar.google.com/citations?user=d4xi2HIAAAAJ&amp;hl=en"
            target="_blank"
            rel="noopener me"
            aria-label="Google Scholar"
            title="Google Scholar"
          >
            
              <i class="ai ai-google-scholar fa-2x" aria-hidden="true"></i>
            
          </a>
        </li>
      
        <li class="sidebar__list-item">
          <a
            href="https://twitter.com/stochasticdoggo"
            target="_blank"
            rel="noopener me"
            aria-label="Twitter"
            title="Twitter"
          >
            
              <i class="fab fa-twitter fa-2x" aria-hidden="true"></i>
            
          </a>
        </li>
      
        <li class="sidebar__list-item">
          <a
            href="https://bsky.app/profile/stochasticdoggo.bsky.social"
            target="_blank"
            rel="noopener me"
            aria-label="Bluesky"
            title="Bluesky"
          >
            
              <svg xmlns="http://www.w3.org/2000/svg"  width="32" height="32" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" vertical-align="middle" class="icon fa-2x">
                <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
                <path d="M6.335 5.144c-1.654 -1.199 -4.335 -2.127 -4.335 .826c0 .59 .35 4.953 .556 5.661c.713 2.463 3.13 2.75 5.444 2.369c-4.045 .665 -4.889 3.208 -2.667 5.41c1.03 1.018 1.913 1.59 2.667 1.59c2 0 3.134 -2.769 3.5 -3.5c.333 -.667 .5 -1.167 .5 -1.5c0 .333 .167 .833 .5 1.5c.366 .731 1.5 3.5 3.5 3.5c.754 0 1.637 -.571 2.667 -1.59c2.222 -2.203 1.378 -4.746 -2.667 -5.41c2.314 .38 4.73 .094 5.444 -2.369c.206 -.708 .556 -5.072 .556 -5.661c0 -2.953 -2.68 -2.025 -4.335 -.826c-2.293 1.662 -4.76 5.048 -5.665 6.856c-.905 -1.808 -3.372 -5.194 -5.665 -6.856z"/>
              </svg>
            
          </a>
        </li>
      
        <li class="sidebar__list-item">
          <a
            href="https://www.linkedin.com/in/katrina-drozdov/"
            target="_blank"
            rel="noopener me"
            aria-label="LinkedIn"
            title="LinkedIn"
          >
            
              <i class="fab fa-linkedin fa-2x" aria-hidden="true"></i>
            
          </a>
        </li>
      
        <li class="sidebar__list-item">
          <a
            href="/Katrina_Drozdov_resume.pdf"
            target="_blank"
            rel="noopener me"
            aria-label="Resume"
            title="Resume"
          >
            
              <i class="fas fa-file-alt fa-2x" aria-hidden="true"></i>
            
          </a>
        </li>
      
    </ul>    
  </div><footer class="footer footer__sidebar">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        Katrina Drozdov
        2025
      
    </li>
    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js"
    integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ="
    crossorigin="anonymous"
  ></script></div>
</aside>
      <main
        
          class="wrapper__main"
        
      >
        <header class="header"><div
  class="
    animated fadeInDown
  "
>
  <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false">
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
  </a>
  <nav class="nav">
    <ul class="nav__list" id="navMenu">
      
      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/about/"
              
              title=""
              >About</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/papers/"
              
              title=""
              >Publications</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/news/"
              
              title=""
              >News</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/posts/"
              
              title=""
              >Posts</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/contact/"
              
              title=""
              >Contact</a
            >
          </li>
        

      
    </ul>
    <ul class="nav__list nav__list--end">
      
      
        <li class="nav__list-item">
          <div class="themeswitch">
            <a title="Switch Theme">
              <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a>
          </div>
        </li>
      
    </ul>
  </nav>
</div>
</header>
  <div
    class="post 
      animated fadeInDown
    "
  >
    
    <div class="post__content">
      
        <h1>9 Lessons I Learned While Doing RL Post-Training for LLMs</h1>
      
      <p>I recently had the chance to experiment with post-training techniques for large language models, a space that has become central to making LLMs useful and controllable in real-world applications. I used <a href="https://arxiv.org/abs/2402.03300">Group Relative Policy Optimization (GRPO)</a> from the open-source <a href="https://github.com/huggingface/open-r1"><code>open-r1</code></a> repository, fine-tuning <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct</a> on the coding subset of the <a href="https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts">Mixture-of-Thoughts</a> dataset.</p>
<p>The experience was very illuminating, especially around stability, reward design, and evaluation. Training a model with pure RL is different from unsupervised and supervised deep learning. It took some trial and error, so to save you some time, here are my top takeaways from training a code generation model with GRPO.</p>
<hr>
<h2 id="1-inputs-and-rewards-are-all-you-need-grpo-is-label-free">1. Inputs and rewards are all you need (GRPO is label-free).</h2>
<p>Reinforcement learning from human (or heuristic) feedback is not just a continuation of pretraining or supervised fine-tuning (SFT). It is <em>policy optimization</em> and the model learns from comparisons, not labels. The training loop must be stable despite reward sparsity, exploration tradeoffs, and longer feedback cycles.</p>
<hr>
<h2 id="2-instability-can-happen-quickly-and-unexpectedly">2. Instability can happen quickly and unexpectedly.</h2>
<p>Some of the failures I saw such as sudden spikes in gradient norms, excessively long completions, or language shifts were symptoms of instability driven by reward noise. When reward signals were inconsistent or sparse, the model made large, erratic updates that could trigger irreversible collapse.</p>
<hr>
<h2 id="3-reward-design-is-crucial-for-success">3. Reward design is crucial for success.</h2>
<p>In GRPO, there is no ground truth as your model is judged purely by the reward function. If the function is not directly aligned with the end goal, the model can learn to generate answers that get a high reward but are not useful. When I trained a model with only a code format reward, I found that it would produce a code block, but the actual code would often fail to execute. Adding a correctness reward was key. Not only would this ensure the code executed, but also that it provided results in the desired format.</p>
<hr>
<h2 id="4-more-generations--larger-batch-size--more-learning-signal">4. More generations + larger batch size = more learning signal.</h2>
<p>In GRPO, <a href="https://github.com/huggingface/trl/blob/15ff54790b42297d2cf569fba6d7dd44c1c269e3/trl/trainer/grpo_config.py#L53"><code>num_generations</code></a> defines how many completions the model produces per prompt. I found that increasing the number of generations, combined with gradient accumulation, produced more stable learning and better reward comparisons. But there’s a tradeoff: too few generations lead to noisy learning, while too many slow down training significantly.</p>
<hr>
<h2 id="5-use-num_iterations-to-stabilize-training-and-improve-efficiency">5. Use <code>num_iterations</code> to stabilize training and improve efficiency.</h2>
<p>The <a href="https://github.com/huggingface/trl/blob/15ff54790b42297d2cf569fba6d7dd44c1c269e3/trl/trainer/grpo_config.py#L135"><code>num_iterations</code></a> hyperparameter determines how many times each set of generated completions is reused for policy updates. Increasing it helped smooth out training, reduced gradient noise, and made better use of GPU compute. More updates per sample means greater training stability and faster progress, without needing to generate more data.</p>
<hr>
<h2 id="6-dr-grpo-can-enhance-learning">6. Dr. GRPO can enhance learning.</h2>
<p><a href="https://arxiv.org/abs/2503.20783">Dr. GRPO</a> improves on the vanilla GRPO approach by increasing token efficiency. It drops the length normalization term, which essentially prevents the model from generating progressively longer incorrect responses. It also drops the KL divergence penalty, which is often unnecessary when using a verifiable reward (unlike in RLHF, where distributional shift is a bigger concern). In my experiments, removing the KL term not only simplified the objective but also reduced memory and compute overhead, leading to faster and more stable training.</p>
<hr>
<h2 id="7-logging-completions-is-a-must">7. Logging completions is a must.</h2>
<p>Quantitative rewards don’t tell the whole story. I caught many issues with training such as super long completions or language switching thanks to inspecting samples during training. High reward scores don’t always mean high-quality responses. Logging even a few completions per step is essential for catching issues early.</p>
<hr>
<h2 id="8-regularization-for-diversity-helps-the-model-learn">8. Regularization for diversity helps the model learn.</h2>
<p>While GRPO is inherently more stable than online RL, it can still converge to repetitive or safe behaviors. Even with multiple completions per prompt, I noticed that the model often produced very similar outputs. That can limit learning. Increasing diversity within generation groups (e.g., with temperature or nucleus sampling) can help the model explore new solution strategies.</p>
<hr>
<h2 id="9-start-with-a-strong-sft-baseline-when-you-can">9. Start with a strong SFT baseline when you can.</h2>
<p>Post-training a base model from scratch is challenging, especially for complex, multi-turn tasks like those in Mixture-of-Thoughts. A base model may struggle to generate useful completions early on, making it less likely to receive any reward signal at all. This can stall learning or lead to instability. In contrast, a supervised fine-tuned (SFT) model starts from more relevant responses, giving GRPO a much better foundation to refine and align behavior effectively.</p>
<hr>
</div>
    <div class="post__footer">
      

      
    </div>

    
  </div>

      </main>
    </div><footer class="footer footer__base">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        Katrina Drozdov
        2025
      
    </li>
    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js"
    integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ="
    crossorigin="anonymous"
  ></script></body>
</html>
