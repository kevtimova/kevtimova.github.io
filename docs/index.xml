<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Katrina Drozdov (Evtimova) | AI Researcher on Katrina Drozdov&#39;s Website</title>
    <link>https://kevtimova.github.io/</link>
    <description>Recent content in Katrina Drozdov (Evtimova) | AI Researcher on Katrina Drozdov&#39;s Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 10 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://kevtimova.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>News</title>
      <link>https://kevtimova.github.io/news/</link>
      <pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>https://kevtimova.github.io/news/</guid>
      
        <description>&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;December 2024:&lt;/strong&gt; Check out my &lt;a href=&#34;https://arxiv.org/abs/2412.10925&#34;&gt;new preprint&lt;/a&gt; on video representation learning with joint-embedding predictive architectures!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;October 2024:&lt;/strong&gt; My research on emergent communication with adaptive compute at inference time was featured in the CDS Blog: &lt;a href=&#34;https://nyudatascience.medium.com/from-academia-to-industry-how-a-2018-paper-foreshadowed-openais-latest-innovation-be9b7b959845&#34;&gt;&amp;ldquo;From Academia to Industry: How a 2018 Paper Foreshadowed OpenAI’s Latest Innovation&amp;rdquo;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;September 2024:&lt;/strong&gt; I&amp;rsquo;ve been invited to serve as a reviewer for the prestigious &lt;a href=&#34;https://jmlr.org/tmlr/&#34;&gt;TMLR&lt;/a&gt; journal.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;July 2024:&lt;/strong&gt; Excited to share that I defended my thesis titled &amp;ldquo;Representation Learning with Regularized Energy-Based Models&amp;rdquo;!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;February 2024:&lt;/strong&gt; Serving as a reviewer at ICML 2024.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;November 2023:&lt;/strong&gt; Serving as a reviewer at AISTATS 2024.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;October 2023:&lt;/strong&gt; Serving as a reviewer at ICLR 2024.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;March 2023:&lt;/strong&gt; Serving as a reviewer at ICML 2023.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;January 2023:&lt;/strong&gt; Gave an invited talk on self-supervised learning at Prof. Leif Weatherby&amp;rsquo;s course &lt;em&gt;&amp;ldquo;Theory of the Digital&amp;rdquo;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;November 2022:&lt;/strong&gt; I&amp;rsquo;ll be at NeurIPS 2022. Would love to chat about self-supervised learning, regularization, latent variable models, etc. If you&amp;rsquo;re also around, do reach out!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;August 2022:&lt;/strong&gt; &lt;a href=&#34;https://openreview.net/forum?id=4GuIi1jJ74&#34;&gt;&amp;ldquo;Sparse Coding with Multi-Layer Decoders using Variance Regularization&amp;rdquo;&lt;/a&gt; is now published at TMLR!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;April 2022:&lt;/strong&gt; Selected as one of the &lt;a href=&#34;https://iclr.cc/Conferences/2022/Reviewers&#34;&gt;Highlighted Reviewers&lt;/a&gt; at ICLR 2022.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;April 2022:&lt;/strong&gt; Serving as a reviewer at ICML 2022.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;October 2021:&lt;/strong&gt; Serving as a reviewer at ICLR 2022.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;September 2021:&lt;/strong&gt; Excited to be an organizer of the &lt;a href=&#34;https://nyu-mll.github.io/nyu-ai-school-2022/&#34;&gt;2022 NYU AI School&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;July 2021:&lt;/strong&gt; Serving as a reviewer at NeurIPS 2021.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;March 2021:&lt;/strong&gt; Serving as a reviewer at ICML 2021.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;February 2021:&lt;/strong&gt; Serving as a reviewer at the Energy-based Models workshop at ICLR 2021.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;April 2020:&lt;/strong&gt; Happy to share that I passed my Depth Qualification Exam on the topic of &lt;em&gt;&amp;ldquo;Energy-Based Learning &amp;amp; Regularized Latent Variable Models&amp;rdquo;&lt;/em&gt; with committee members Joan Bruna, Kyunghyun Cho, and Yann LeCun.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;January 2020:&lt;/strong&gt; Gave a talk at the &lt;a href=&#34;https://cims.nyu.edu/ai/seminars/cilvr-seminar-series/&#34;&gt;CILVR seminar&lt;/a&gt; titled &lt;em&gt;&amp;ldquo;Self-supervised Learning &amp;amp; Sparse Overcomplete Representations of Visual Data&amp;rdquo;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;January 2020:&lt;/strong&gt; Looking forward to being a teaching assistant for &lt;em&gt;Introduction to Machine Learning&lt;/em&gt; at Courant over the spring.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;February 2019:&lt;/strong&gt; Excited to share that I&amp;rsquo;ll be interning at &lt;a href=&#34;https://research.fb.com/category/facebook-ai-research/&#34;&gt;FAIR&lt;/a&gt; this summer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;January 2019:&lt;/strong&gt; Happy to be a section leader for NYU&amp;rsquo;s Deep Learning class this spring.&lt;/li&gt;
&lt;/ul&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>https://kevtimova.github.io/publications/papers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kevtimova.github.io/publications/papers/</guid>
      
        <description>&lt;p&gt;&lt;strong&gt;Video Representation Learning with Joint-Embedding Predictive Architectures&lt;/strong&gt;. &lt;strong&gt;Drozdov, K.&lt;/strong&gt;, Shwartz-Ziv, R. and LeCun, Y. Preprint, 2024. &lt;a href=&#34;https://arxiv.org/pdf/2412.10925.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>About Me</title>
      <link>https://kevtimova.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kevtimova.github.io/about/</guid>
      
        <description>&lt;p&gt;I earned my PhD from NYU&amp;rsquo;s Center for Data Science, where I was advised by Yann LeCun. My research focuses on self-supervised learning methods for extracting meaningful data representations. In particular, I develop regularization techniques that prevent collapse during model training, ensuring that the learned representations remain informative and useful.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Contact</title>
      <link>https://kevtimova.github.io/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kevtimova.github.io/contact/</guid>
      
        <description>&lt;p&gt;✉️ kve216 at nyu.edu&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>